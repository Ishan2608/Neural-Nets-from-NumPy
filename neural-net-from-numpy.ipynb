{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Neural Network Implementation using NumPy\nIn this guide, we'll walk through building a neural network from scratch using NumPy, trained on the MNIST dataset. We will preprocess the data using Pandas, and then construct and train the neural network using NumPy. The MNIST dataset is a collection of 28x28 grayscale images of handwritten digits (0-9).","metadata":{}},{"cell_type":"markdown","source":"## Import Statements","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:32:20.718978Z","iopub.execute_input":"2024-06-12T14:32:20.719924Z","iopub.status.idle":"2024-06-12T14:32:20.724896Z","shell.execute_reply.started":"2024-06-12T14:32:20.719889Z","shell.execute_reply":"2024-06-12T14:32:20.723635Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"## Load the required Dataset\nLoad the MNIST dataset and preprocess it. The training dataset contains labels in the first column, while the test dataset does not. The rest columns represent each pixel, thus each row represents an image.","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\nprint(f\"Shape of Train Dataset: {train_data.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:32:24.533445Z","iopub.execute_input":"2024-06-12T14:32:24.533855Z","iopub.status.idle":"2024-06-12T14:32:27.708468Z","shell.execute_reply.started":"2024-06-12T14:32:24.533821Z","shell.execute_reply":"2024-06-12T14:32:27.707331Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stdout","text":"Shape of Train Dataset: (42000, 785)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Train Dataset**","metadata":{}},{"cell_type":"code","source":"train_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T13:59:37.952665Z","iopub.execute_input":"2024-06-12T13:59:37.953062Z","iopub.status.idle":"2024-06-12T13:59:37.972370Z","shell.execute_reply.started":"2024-06-12T13:59:37.953032Z","shell.execute_reply":"2024-06-12T13:59:37.971101Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 785 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"**Features and Label**\n\nSplit the training dataset into features and label set. To make the calculations easy, normalize the dataset by dividing the values by 255. This ensures that the values are in the range 0 to 1.","metadata":{}},{"cell_type":"code","source":"# Shuffle the rows of the dataset\nshuffled_train_data = train_data.sample(frac=1, random_state=42)\n\n# Separate features (X) and labels (y)\nX = shuffled_train_data.drop('label', axis=1)\ny = shuffled_train_data['label']\n\n# Split the dataset into training and testing sets\ntrain_size = int(0.9 * len(train_data))\nX_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\ny_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n\nprint(f\"Shape of Training: X -> {X_train.shape}, y -> {y_train.shape}\")\nprint(f\"Shape of Test: X -> {X_test.shape}, y-> {y_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:32:32.464598Z","iopub.execute_input":"2024-06-12T14:32:32.465406Z","iopub.status.idle":"2024-06-12T14:32:32.741376Z","shell.execute_reply.started":"2024-06-12T14:32:32.465369Z","shell.execute_reply":"2024-06-12T14:32:32.740085Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stdout","text":"Shape of Training: X -> (37800, 784), y -> (37800,)\nShape of Test: X -> (4200, 784), y-> (4200,)\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:21:39.417876Z","iopub.execute_input":"2024-06-12T14:21:39.418744Z","iopub.status.idle":"2024-06-12T14:21:39.438520Z","shell.execute_reply.started":"2024-06-12T14:21:39.418706Z","shell.execute_reply":"2024-06-12T14:21:39.437312Z"},"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"       pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n5457        0       0       0       0       0       0       0       0       0   \n38509       0       0       0       0       0       0       0       0       0   \n25536       0       0       0       0       0       0       0       0       0   \n31803       0       0       0       0       0       0       0       0       0   \n39863       0       0       0       0       0       0       0       0       0   \n\n       pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n5457        0  ...         0         0         0         0         0   \n38509       0  ...         0         0         0         0         0   \n25536       0  ...         0         0         0         0         0   \n31803       0  ...         0         0         0         0         0   \n39863       0  ...         0         0         0         0         0   \n\n       pixel779  pixel780  pixel781  pixel782  pixel783  \n5457          0         0         0         0         0  \n38509         0         0         0         0         0  \n25536         0         0         0         0         0  \n31803         0         0         0         0         0  \n39863         0         0         0         0         0  \n\n[5 rows x 784 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>pixel9</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5457</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>38509</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25536</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>31803</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>39863</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 784 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X_test.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:21:43.904491Z","iopub.execute_input":"2024-06-12T14:21:43.904907Z","iopub.status.idle":"2024-06-12T14:21:43.925660Z","shell.execute_reply.started":"2024-06-12T14:21:43.904873Z","shell.execute_reply":"2024-06-12T14:21:43.924615Z"},"trusted":true},"execution_count":83,"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"       pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n33092       0       0       0       0       0       0       0       0       0   \n30563       0       0       0       0       0       0       0       0       0   \n17064       0       0       0       0       0       0       0       0       0   \n16679       0       0       0       0       0       0       0       0       0   \n30712       0       0       0       0       0       0       0       0       0   \n\n       pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n33092       0  ...         0         0         0         0         0   \n30563       0  ...         0         0         0         0         0   \n17064       0  ...         0         0         0         0         0   \n16679       0  ...         0         0         0         0         0   \n30712       0  ...         0         0         0         0         0   \n\n       pixel779  pixel780  pixel781  pixel782  pixel783  \n33092         0         0         0         0         0  \n30563         0         0         0         0         0  \n17064         0         0         0         0         0  \n16679         0         0         0         0         0  \n30712         0         0         0         0         0  \n\n[5 rows x 784 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>pixel9</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>33092</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30563</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17064</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16679</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30712</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 784 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"y_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:21:48.699539Z","iopub.execute_input":"2024-06-12T14:21:48.700534Z","iopub.status.idle":"2024-06-12T14:21:48.707872Z","shell.execute_reply.started":"2024-06-12T14:21:48.700501Z","shell.execute_reply":"2024-06-12T14:21:48.706751Z"},"trusted":true},"execution_count":84,"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"5457     8\n38509    1\n25536    9\n31803    9\n39863    8\nName: label, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"y_test.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:21:50.612334Z","iopub.execute_input":"2024-06-12T14:21:50.612726Z","iopub.status.idle":"2024-06-12T14:21:50.620904Z","shell.execute_reply.started":"2024-06-12T14:21:50.612689Z","shell.execute_reply":"2024-06-12T14:21:50.619730Z"},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"33092    5\n30563    2\n17064    4\n16679    9\n30712    0\nName: label, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Ensure the data is in numpy array format\nX_train, X_test = np.array(X_train), np.array(X_test)\ny_train, y_test = np.array(y_train), np.array(y_test)\n\n# Normalize the data\nX_train = X_train / 255.0\nX_test = X_test / 255.0\nprint(f\"Shape of X_train: {X_train.shape} and X_test: {X_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:32:41.993489Z","iopub.execute_input":"2024-06-12T14:32:41.993894Z","iopub.status.idle":"2024-06-12T14:32:42.252803Z","shell.execute_reply.started":"2024-06-12T14:32:41.993863Z","shell.execute_reply":"2024-06-12T14:32:42.251704Z"},"trusted":true},"execution_count":106,"outputs":[{"name":"stdout","text":"Shape of X_train: (37800, 784) and X_test: (4200, 784)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**One-Hot Encoding**\n\nIt is a process that transforms categorical labels into a format that can be provided to machine learning algorithms to do a better job in prediction. For the MNIST dataset, where we have labels for digits 0 through 9, one-hot encoding will convert each label into a binary vector of length 10. Each position in the vector corresponds to one of the digits, with a 1 indicating the presence of the digit and 0s everywhere else.\n\nSteps:\n1. We first create an array of zeros with the shape (number of samples, number of classes). For the MNIST dataset, we have 42,000 samples and 10 classes (digits 0-9).\n  - `y_train_one_hot = np.zeros((y_train.size, y_train.max() + 1))`\n  - `y_train.size` gives the number of samples (42,000). <br>\n  - `y_train.max() + 1` gives the number of classes (10). We add 1 because y_train.max() gives the highest label, which is 9, and we need a total of 10 classes (0-9).\n2. We use NumPy's advanced indexing to set the appropriate positions to 1. For each sample, we set the column corresponding to its label to 1. <br>\n  - `y_train_one_hot[np.arange(y_train.size), y_train] = 1` <br>\n  - `np.arange(y_train.size)` generates an array of indices from 0 to 41,999 (42,000 samples).\n  - `y_train` is the array of labels, where each element is a digit between 0 and 9.\n  - `np.arange(y_train.size), y_train` is used to index into the y_train_one_hot array. This line sets the appropriate column for each row to 1.\n`","metadata":{}},{"cell_type":"code","source":"# One-hot encode the labels for training set\nrows_train = y_train.size\ncols_train = y_train.max()\ny_train_one_hot = np.zeros((rows_train, cols_train + 1))\ny_train_one_hot[np.arange(rows_train), y_train] = 1\n\n# One-hot encode the labels for test set\nrows_test = y_test.size\ncols_test = y_test.max()\ny_test_one_hot = np.zeros((rows_test, cols_test + 1))\ny_test_one_hot[np.arange(rows_test), y_test] = 1","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:32:51.575884Z","iopub.execute_input":"2024-06-12T14:32:51.576322Z","iopub.status.idle":"2024-06-12T14:32:51.585409Z","shell.execute_reply.started":"2024-06-12T14:32:51.576288Z","shell.execute_reply":"2024-06-12T14:32:51.584117Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"print(f\"{y_train_one_hot.shape}\")\nprint(f\"{y_test_one_hot.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:32:54.682110Z","iopub.execute_input":"2024-06-12T14:32:54.683068Z","iopub.status.idle":"2024-06-12T14:32:54.688534Z","shell.execute_reply.started":"2024-06-12T14:32:54.683027Z","shell.execute_reply":"2024-06-12T14:32:54.687268Z"},"trusted":true},"execution_count":108,"outputs":[{"name":"stdout","text":"(37800, 10)\n(4200, 10)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Initialize Neural Networks\n\nIn this step, we define the structure of our neural network and initialize the weights and biases for the layers. Letâ€™s break this down in great detail.\n\n**Weight Initialization**\n\nWeights are the parameters that the neural network learns during training. We need to initialize them with small random values.\n\n- np.random.seed(42): Sets the random seed for reproducibility, ensuring the same random values are generated each time we run the code.\n- np.random.randn(input_size, hidden_size) * 0.01: Generates a matrix of random values from a standard normal distribution (mean = 0, standard deviation = 1). Multiplying by 0.01 scales these values down to be smaller, which helps with the convergence of the training process.\n- W1 is the weight matrix for the connections between the input layer and the hidden layer. Its shape is (784, 64).\n- W2 is the weight matrix for the connections between the hidden layer and the output layer. Its shape is (64, 10).\n\n**Why Bias is a Matrix with Only 1 Row**\n\nBiases are typically vectors (or 1D arrays) with one bias value per neuron in the layer they are associated with. In our code, they are initialized as matrices with one row to facilitate matrix operations during the forward and backward propagation steps.\n\n**Matrix Shapes:**\n\n- Weights (W1 and W2):\n\n  W1 has a shape of (784, 64), meaning each of the 784 input features has 64 weights connecting it to the 64 neurons in the hidden layer.\n  W2 has a shape of (64, 10), meaning each of the 64 hidden neurons has 10 weights connecting it to the 10 output neurons.\n\n**Biases (b1 and b2):**\n\n- b1 has a shape of (1, 64), meaning there is one bias value for each of the 64 neurons in the hidden layer.\n- b2 has a shape of (1, 10), meaning there is one bias value for each of the 10 output neurons.\n\nBy having biases as 2D arrays with one row, it makes it easier to add them to the weighted sums during forward propagation using broadcasting in NumPy.","metadata":{}},{"cell_type":"code","source":"# Define the neural network structure\ninput_size = 784  # 28x28 pixels flattened\nhidden_size = 64  # Number of hidden neurons\noutput_size = 10  # Number of classes (digits 0-9)\n\n# Initialize weights and biases\nnp.random.seed(42)\nW1 = np.random.randn(input_size, hidden_size) * 0.01\nb1 = np.zeros((1, hidden_size))\nW2 = np.random.randn(hidden_size, output_size) * 0.01\nb2 = np.zeros((1, output_size))\nprint(f\"Shape of X_train: {X_train.shape}\")\nprint(f\"Shape of y_train: {y_train.shape}\")\nprint(f\"Shape of X_test: {X_test.shape}\")\nprint(f\"Shape of y_test: {y_test.shape}\")\nprint(f\"Shape of W1: {W1.shape}, b1: {b1.shape}\")\nprint(f\"Shape of W2: {W2.shape}, b2: {b2.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:32:59.770416Z","iopub.execute_input":"2024-06-12T14:32:59.771221Z","iopub.status.idle":"2024-06-12T14:32:59.781391Z","shell.execute_reply.started":"2024-06-12T14:32:59.771190Z","shell.execute_reply":"2024-06-12T14:32:59.780324Z"},"trusted":true},"execution_count":109,"outputs":[{"name":"stdout","text":"Shape of X_train: (37800, 784)\nShape of y_train: (37800,)\nShape of X_test: (4200, 784)\nShape of y_test: (4200,)\nShape of W1: (784, 64), b1: (1, 64)\nShape of W2: (64, 10), b2: (1, 10)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define Activation Functions","metadata":{}},{"cell_type":"code","source":"def relu(Z):\n    return np.maximum(0, Z)\n\ndef relu_derivative(Z):\n    return Z > 0\n\ndef softmax(Z):\n    expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n    return expZ / np.sum(expZ, axis=1, keepdims=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:33:03.961644Z","iopub.execute_input":"2024-06-12T14:33:03.962076Z","iopub.status.idle":"2024-06-12T14:33:03.969093Z","shell.execute_reply.started":"2024-06-12T14:33:03.962044Z","shell.execute_reply":"2024-06-12T14:33:03.967687Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":"## Step 5: Forward Propagation\n\nForward propagation is the process of calculating the output of the neural network by passing the input data through each layer. Hereâ€™s a detailed breakdown of each step involved.\n\n### What Happens During Forward Propagation?\n\n1. Calculate the input to the hidden layer neurons (Z1)\n2. Apply the activation function to compute the hidden layer activations (A1)\n3. Calculate the input to the output layer neurons (Z2)\n4. Apply the softmax activation function to compute the output layer activations (A2)\n\n### Detailed Explanation\n\n#### Weight Matrix Shapes and Connections\n\n**Weights Matrix:**\n\n- The weight matrix $(W1)$ connects the input layer to the hidden layer.\n  - Each neuron in the hidden layer receives input from every neuron in the input layer.\n  - Hence, if the input layer has 784 neurons (features) and the hidden layer has 64 neurons, $(W1)$ will be of shape $( (784, 64))$.\n  - Each element $( W1_{ij})$ represents the weight of the connection from the $( i^{th} )$ input feature to the $( j^{th})$ hidden neuron.\n\n- The weight matrix $(W2)$ connects the hidden layer to the output layer.\n  - Each neuron in the output layer receives input from every neuron in the hidden layer.\n  - Hence, if the hidden layer has 64 neurons and the output layer has 10 neurons (one for each digit), $(W2)$ will be of shape $( (64, 10))$.\n  - Each element $( W2_{ij} )$ represents the weight of the connection from the $( i^{th})$ hidden neuron to the $( j^{th} )$ output neuron.\n\n#### Biases\n\n- The biases $(b1)$ and $(b2)$ are vectors (represented as 1-row matrices here) that add a constant to the activation of each neuron in the hidden and output layers, respectively.\n- $(b1)$ has shape $( (1, 64) )$ and $(b2)$ has shape $( (1, 10) )$.\n\n### Step-by-Step Process\n\n#### 1. Input to Hidden Layer (Z1)\n\n**Matrix Multiplication and Bias Addition:**\n\n```python\nZ1 = np.dot(X, W1) + b1\n```\n\n- $(X)$ has shape $( (number\\_of\\_samples, 784) )$.\n- $(W1)$ has shape $( (784, 64) )$.\n- $(b1)$ has shape $( (1, 64) )$.\n\n**Why these shapes?**\n\n- When you multiply $(X)$ (which has one row per sample and one column per input feature) by $(W1)$ (which has one column per hidden neuron), each sampleâ€™s input features are combined linearly to produce the hidden layer inputs.\n- Adding $(b1)$ broadcasts the bias term to each of the samples, ensuring each hidden neuron has its bias added to its input.\n\n#### 2. Activation Function for Hidden Layer (A1)\n\n**Apply ReLU Activation:**\n\n```python\nA1 = relu(Z1)\n```\n\n- The ReLU function applies element-wise, setting all negative values to 0, thereby introducing non-linearity.\n\n#### 3. Hidden Layer to Output Layer (Z2)\n\n**Matrix Multiplication and Bias Addition:**\n\n```python\nZ2 = np.dot(A1, W2) + b2\n```\n\n- $(A1)$ has shape $( (number\\_of\\_samples, 64) )$.\n- $(W2)$ has shape $( (64, 10) )$.\n- $(b2)$ has shape $( (1, 10) )$.\n\n**Why these shapes?**\n\n- When you multiply $(A1)$ (which has one row per sample and one column per hidden neuron) by $(W2)$ (which has one column per output neuron), each hidden neuronâ€™s activations are combined linearly to produce the output layer inputs.\n- Adding $(b2)$ broadcasts the bias term to each of the samples, ensuring each output neuron has its bias added to its input.\n\n#### 4. Activation Function for Output Layer (A2)\n\n**Apply Softmax Activation:**\n\n```python\nA2 = softmax(Z2)\n```\n\n- The softmax function converts the output layer inputs into probabilities, where each value in the output vector represents the probability of the sample belonging to each class.\n\n### Visualization of Weight Matrices\n\n**Visualizing Weight Matrix $(W1)$:**\n\n- Imagine $(W1)$ as a table where:\n  - Rows correspond to input features (784 in total).\n  - Columns correspond to hidden neurons (64 in total).\n- Each entry $(W1_{ij})$ is the weight from the $(i^{th})$ input feature to the $(j^{th})$ hidden neuron.\n\n**Visualizing Weight Matrix $(W2)$:**\n\n- Imagine $(W2)$ as a table where:\n  - Rows correspond to hidden neurons (64 in total).\n  - Columns correspond to output neurons (10 in total).\n- Each entry $(W2_{ij})$ is the weight from the $(i^{th})$ hidden neuron to the $(j^{th})$ output neuron.\n\n### Summary\n\n- **Weight Matrices:** Essential for connecting neurons between layers. Each connection (input feature to hidden neuron, hidden neuron to output neuron) has a unique weight.\n- **Bias Vectors:** Provide an additional parameter for each neuron to ensure the neuron has flexibility in its output.\n- **Forward Propagation:** Involves matrix multiplication and adding biases to compute neuron inputs, followed by applying activation functions to compute neuron outputs.\n\nBy structuring the weights and biases as matrices, we efficiently utilize matrix operations to handle multiple samples in parallel, which is crucial for efficient training and inference in neural networks.","metadata":{}},{"cell_type":"code","source":"def forward_propagation(X, W1, b1, W2, b2):\n    Z1 = np.dot(X, W1) + b1\n    A1 = relu(Z1)\n    Z2 = np.dot(A1, W2) + b2\n    A2 = softmax(Z2)\n    return Z1, A1, Z2, A2\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:33:10.045790Z","iopub.execute_input":"2024-06-12T14:33:10.046204Z","iopub.status.idle":"2024-06-12T14:33:10.052239Z","shell.execute_reply.started":"2024-06-12T14:33:10.046172Z","shell.execute_reply":"2024-06-12T14:33:10.051223Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"## Define the Loss Function","metadata":{}},{"cell_type":"code","source":"def compute_loss(y_true, y_pred):\n    m = y_true.shape[0]\n    log_likelihood = -np.log(y_pred[range(m), y_true.argmax(axis=1)])\n    loss = np.sum(log_likelihood) / m\n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:33:13.646201Z","iopub.execute_input":"2024-06-12T14:33:13.646625Z","iopub.status.idle":"2024-06-12T14:33:13.652786Z","shell.execute_reply.started":"2024-06-12T14:33:13.646593Z","shell.execute_reply":"2024-06-12T14:33:13.651483Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"markdown","source":"## Backward Propagation\n\nBackward propagation, or backpropagation, is the process of computing the gradient of the loss function with respect to each parameter (weights and biases) in the network. This gradient is then used to update the parameters to minimize the loss.\n\nHereâ€™s a detailed breakdown of the backward propagation step.\n\n### Detailed Steps in Backward Propagation\n\n1. **Compute the Loss Gradient with Respect to the Output Layer (dZ2)**\n2. **Gradient of the Loss with Respect to Weights and Biases of the Output Layer (dW2, db2)**\n3. **Gradient of the Loss with Respect to the Hidden Layer Activations (dA1)**\n4. **Gradient of the Loss with Respect to the Hidden Layer Pre-Activations (dZ1)**\n5. **Gradient of the Loss with Respect to Weights and Biases of the Hidden Layer (dW1, db1)**\n6. **Update the Weights and Biases Using the Gradients**\n\nLet's break each of these steps down in detail:\n\n### 1. Compute the Loss Gradient with Respect to the Output Layer (dZ2)\n\nFirst, we calculate the gradient of the loss with respect to the output layer activations $( A2 )$.\n\n#### Loss Gradient (Cross-Entropy Loss)\n\nFor a classification problem with cross-entropy loss, the gradient of the loss with respect to the output activations $( A2 )$ is:\n\n$$[\ndZ2 = A2 - Y\n]$$\n\nHere:\n- $( A2 )$ is the output from the softmax activation (predicted probabilities), shape $( (42000, 10) )$.\n- $( Y )$ is the one-hot encoded true labels, shape $( (42000, 10) )$.\n- $( dZ2 )$ is the gradient of the loss with respect to $( Z2 )$ (pre-activation output of the output layer), shape $( (42000, 10) )$.\n\n### 2. Gradient of the Loss with Respect to Weights and Biases of the Output Layer (dW2, db2)\n\nNext, we compute the gradients of the loss with respect to the weights $( W2 )$ and biases $( b2 )$.\n\n#### Weight Gradient (dW2)\n\n$$[\ndW2 = \\frac{1}{m} A1^T \\cdot dZ2\n]$$\n\n- $( A1^T )$ is the transpose of $( A1 )$, shape $( (64, 42000) )$.\n- $( dZ2 )$ is the gradient from the previous step, shape $( (42000, 10) )$.\n- $( dW2 )$ is the gradient of the loss with respect to $( W2 )$, shape $( (64, 10) )$.\n- $( m )$ is the number of samples (42,000).\n\n#### Bias Gradient (db2)\n\n$$[\ndb2 = \\frac{1}{m} \\sum_{i=1}^m dZ2_i\n]$$\n\n- $( db2 )$ is the gradient of the loss with respect to $( b2 )$, shape $( (1, 10) )$.\n- This is calculated by summing $( dZ2 )$ over all samples and then averaging.\n\n### 3. Gradient of\n\n#### Gradient of the Loss with Respect to Hidden Layer Activations (dA1)\n\nNext, we need to propagate the gradient back to the hidden layer. We start by computing the gradient of the loss with respect to the hidden layer activations $( A1 )$.\n\n$$[\ndA1 = dZ2 \\cdot W2^T\n]$$\n\n- $( dZ2 )$ is the gradient from the previous step, shape $( (42000, 10) )$.\n- $( W2^T )$ is the transpose of $( W2 )$, shape $( (10, 64) )$.\n- $( dA1 )$ is the gradient of the loss with respect to the hidden layer activations, shape $( (42000, 64) )$.\n\n#### Gradient of the Loss with Respect to Hidden Layer Pre-Activations (dZ1)\n\nTo compute the gradient of the loss with respect to the hidden layer pre-activations $( Z1 )$, we need to consider the activation function used in the hidden layer. Assuming we used the ReLU activation function:\n\n$$[\ndZ1 = dA1 \\cdot \\text{ReLU}'(Z1)\n]$$\n\n- $( dA1 )$ is the gradient from the previous step, shape $( (42000, 64) )$.\n- $( \\text{ReLU}'(Z1) )$ is the derivative of the ReLU function applied element-wise to $( Z1 )$. The ReLU derivative is 1 for positive $( Z1 )$ values and 0 for non-positive $( Z1 )$ values.\n- $( dZ1 )$ is the gradient of the loss with respect to the hidden layer pre-activations, shape $( (42000, 64) )$.\n\n#### Gradient of the Loss with Respect to Weights and Biases of the Hidden Layer (dW1, db1)\n\nNow, we compute the gradients of the loss with respect to the weights $( W1 )$ and biases $( b1 )$ of the hidden layer.\n\n#### Weight Gradient (dW1)\n\n$$[\ndW1 = \\frac{1}{m} X^T \\cdot dZ1\n]$$\n\n- $( X^T )$ is the transpose of the input matrix $( X )$, shape $( (784, 42000) )$.\n- $( dZ1 )$ is the gradient from the previous step, shape $( (42000, 64) )$.\n- $( dW1 )$ is the gradient of the loss with respect to $( W1 )$, shape $( (784, 64) )$.\n- $( m )$ is the number of samples (42,000).\n\n#### Bias Gradient (db1)\n\n$$[\ndb1 = \\frac{1}{m} \\sum_{i=1}^m dZ1_i\n]$$\n\n- $( db1 )$ is the gradient of the loss with respect to $( b1 )$, shape $( (1, 64) )$.\n- This is calculated by summing $( dZ1 )$ over all samples and then averaging.\n\n### 4. Update the Weights and Biases Using the Gradients\n\nFinally, we use the gradients computed to update the weights and biases. This is typically done using gradient descent or one of its variants (e.g., SGD, Adam).\n\n$$[\nW1 = W1 - \\eta \\cdot dW1\n]$$\n$$[\nb1 = b1 - \\eta \\cdot db1\n]$$\n$$[\nW2 = W2 - \\eta \\cdot dW2\n]$$\n$$[\nb2 = b2 - \\eta \\cdot db2\n]$$\n\n- $( \\eta )$ is the learning rate, a hyperparameter that controls the step size of the update.\n- $( W1, b1, W2, )$ and $( b2 )$ are updated using their respective gradients.\n\n### Summary\n\n1. **Compute $( dZ2 )$**: Gradient of the loss with respect to the output layer pre-activations.\n2. **Compute $( dW2 )$ and $( db2 )$**: Gradients of the loss with respect to the weights and biases of the output layer.\n3. **Compute $( dA1 )$**: Gradient of the loss with respect to the hidden layer activations.\n4. **Compute $( dZ1 )$**: Gradient of the loss with respect to the hidden layer pre-activations.\n5. **Compute $( dW1 )$ and $( db1 )$**: Gradients of the loss with respect to the weights and biases of the hidden layer.\n6. **Update Parameters**: Use the computed gradients to update the weights and biases.\n\nBy carefully computing and propagating these gradients backward through the network, we adjust the parameters to minimize the loss, effectively training the neural network.","metadata":{}},{"cell_type":"code","source":"def backward_propagation(X, y_true, Z1, A1, Z2, A2, W1, W2):\n    m = X.shape[0]\n\n    dZ2 = A2 - y_true\n    dW2 = np.dot(A1.T, dZ2) / m\n    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n\n    dA1 = np.dot(dZ2, W2.T)\n    dZ1 = dA1 * relu_derivative(Z1)\n    dW1 = np.dot(X.T, dZ1) / m\n    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n\n    return dW1, db1, dW2, db2\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:33:22.026434Z","iopub.execute_input":"2024-06-12T14:33:22.026881Z","iopub.status.idle":"2024-06-12T14:33:22.035000Z","shell.execute_reply.started":"2024-06-12T14:33:22.026847Z","shell.execute_reply":"2024-06-12T14:33:22.033760Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"markdown","source":"## Update Parameters","metadata":{}},{"cell_type":"code","source":"def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n    W1 -= learning_rate * dW1\n    b1 -= learning_rate * db1\n    W2 -= learning_rate * dW2\n    b2 -= learning_rate * db2\n    return W1, b1, W2, b2\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:33:25.795449Z","iopub.execute_input":"2024-06-12T14:33:25.795846Z","iopub.status.idle":"2024-06-12T14:33:25.801963Z","shell.execute_reply.started":"2024-06-12T14:33:25.795817Z","shell.execute_reply":"2024-06-12T14:33:25.800797Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":"## Train the Neural Network\n\nTraining a neural network involves iteratively updating its parameters (weights and biases) to minimize the error between its predictions and the actual labels in the dataset. This is done over multiple cycles called epochs. Let's go through this step-by-step:\n\n### Steps to Train the Neural Network\n\n1. **Initialize Parameters**: \n   - Initialize the weights and biases of the network with small random values.\n\n2. **Specify Hyperparameters**:\n   - Set the number of epochs (iterations over the entire dataset).\n   - Set the learning rate, which determines how much to adjust the weights and biases in each step.\n\n3. **Epochs Loop**: \n   - For each epoch, perform the following steps:\n\n### Detailed Breakdown of Each Epoch\n\n#### a. Forward Propagation\n\n1. **Input Data**:\n   - Start with the input data \\( X \\) (shape \\( (42000, 784) \\)).\n\n2. **Hidden Layer Calculation**:\n   - Compute \\( Z1 = X \\cdot W1 + b1 \\) to get the pre-activation values for the hidden layer.\n   - Apply the activation function (e.g., ReLU) to get \\( A1 = \\text{ReLU}(Z1) \\).\n\n3. **Output Layer Calculation**:\n   - Compute \\( Z2 = A1 \\cdot W2 + b2 \\) to get the pre-activation values for the output layer.\n   - Apply the softmax activation function to get \\( A2 = \\text{softmax}(Z2) \\), which gives the predicted probabilities for each class.\n\n#### b. Compute Loss\n\n- Calculate the loss using the cross-entropy loss function, which measures how well the predicted probabilities match the actual labels.\n\n#### c. Backward Propagation\n\n1. **Compute Gradients**:\n   - Compute the gradient of the loss with respect to the output layer activations \\( dZ2 = A2 - Y \\).\n   - Compute \\( dW2 \\) and \\( db2 \\) for the output layer using \\( dZ2 \\).\n   - Compute the gradient of the loss with respect to the hidden layer activations \\( dA1 = dZ2 \\cdot W2^T \\).\n   - Compute \\( dZ1 = dA1 \\cdot \\text{ReLU}'(Z1) \\).\n   - Compute \\( dW1 \\) and \\( db1 \\) for the hidden layer using \\( dZ1 \\).\n\n2. **Update Parameters**:\n   - Update \\( W1 \\) and \\( b1 \\) using the computed gradients and the learning rate.\n   - Update \\( W2 \\) and \\( b2 \\) similarly.\n\n#### d. (Optional) Track Performance\n\n- Calculate and store the accuracy or other performance metrics to monitor the training progress.\n\n#### e. Repeat for All Epochs\n\n- Repeat the steps above for the specified number of epochs.\n\n### Summary\n\nHere's a simplified step-by-step summary:\n\n1. **Initialize** the networkâ€™s weights and biases.\n2. **Set** the number of epochs and the learning rate.\n3. **For each epoch**:\n   - Perform **forward propagation** to get predictions.\n   - **Compute the loss** to see how far off the predictions are.\n   - Perform **backward propagation** to compute gradients.\n   - **Update weights and biases** to reduce the loss.\n   - (Optionally) **Track performance** metrics like accuracy.\n4. **Repeat** until the specified number of epochs is complete.\n\nBy iterating through this process, the neural network's weights and biases are adjusted to minimize the loss, effectively learning from the training data.","metadata":{}},{"cell_type":"code","source":"def train_neural_network(X_train, y_train_one_hot, W1, b1, W2, b2, epochs, learning_rate):\n    for epoch in range(epochs):\n        Z1, A1, Z2, A2 = forward_propagation(X_train, W1, b1, W2, b2)\n        loss = compute_loss(y_train_one_hot, A2)\n        dW1, db1, dW2, db2 = backward_propagation(X_train, y_train_one_hot, Z1, A1, Z2, A2, W1, W2)\n        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}, Loss: {loss}\")\n    return W1, b1, W2, b2\n\n# Hyperparameters\nepochs = 201\nlearning_rate = 0.09\n\n# Train the neural network\nW1, b1, W2, b2 = train_neural_network(X_train, y_train_one_hot, W1, b1, W2, b2, epochs, learning_rate)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:34:50.366193Z","iopub.execute_input":"2024-06-12T14:34:50.366609Z","iopub.status.idle":"2024-06-12T14:35:54.770534Z","shell.execute_reply.started":"2024-06-12T14:34:50.366577Z","shell.execute_reply":"2024-06-12T14:35:54.769067Z"},"trusted":true},"execution_count":116,"outputs":[{"name":"stdout","text":"Epoch 0, Loss: 0.5720182501486986\nEpoch 10, Loss: 0.5509250766847066\nEpoch 20, Loss: 0.532377147408783\nEpoch 30, Loss: 0.5159414338013452\nEpoch 40, Loss: 0.5012743096410637\nEpoch 50, Loss: 0.4881077642361666\nEpoch 60, Loss: 0.4762275787002044\nEpoch 70, Loss: 0.4654538720982976\nEpoch 80, Loss: 0.45564197035410714\nEpoch 90, Loss: 0.446670030384761\nEpoch 100, Loss: 0.4384367616040583\nEpoch 110, Loss: 0.43085351353460444\nEpoch 120, Loss: 0.4238463698905928\nEpoch 130, Loss: 0.4173521716633765\nEpoch 140, Loss: 0.41131607716026203\nEpoch 150, Loss: 0.40569026614970505\nEpoch 160, Loss: 0.4004316822171599\nEpoch 170, Loss: 0.39550246272946565\nEpoch 180, Loss: 0.3908692891348406\nEpoch 190, Loss: 0.38650438746297233\nEpoch 200, Loss: 0.3823808331005157\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Make Predictions\n\nIn the provided code, the function `predict` takes the input features `X` along with the learned parameters of the neural network (`W1`, `b1`, `W2`, `b2`) and returns the predictions made by the neural network.\n\nHere's a breakdown of what's happening inside the function:\n\n1. **Forward Propagation**: The function calls the `forward_propagation` function, passing the input features `X` and the learned parameters `W1`, `b1`, `W2`, and `b2`. This function computes the forward pass of the neural network, which involves propagating the input features through the network's layers to generate predictions.\n\n2. **Unpacking Outputs**: The `forward_propagation` function returns four values: `Z1`, `A1`, `Z2`, and `A2`. However, since we are only interested in the final activation values `A2`, we use the underscore `_` to discard the other outputs.\n\n3. **Argmax**: The function then uses NumPy's `argmax` function to find the index of the maximum value along the specified axis (`axis=1`). This effectively selects the neuron with the highest activation value in the output layer for each sample in the input.\n\n4. **Return**: Finally, the function returns the index of the neuron with the highest activation value for each sample in the input. These indices represent the predicted classes for each input sample.\n\nIn summary, the `predict` function takes input features and learned parameters, performs forward propagation to generate predictions, and returns the predicted class labels for each input sample.","metadata":{}},{"cell_type":"code","source":"def predict(X, W1, b1, W2, b2):\n    Z1, A1, Z2, A2 = forward_propagation(X, W1, b1, W2, b2)\n    print(f\"Shapes: Z1 -> {Z1.shape}, A1 -> {A1.shape}, Z2 -> {Z2.shape}, A2 -> {A2.shape}\")\n    return np.argmax(A2, axis=1)\n\n# Make predictions on the test set\npredictions = predict(X_test, W1, b1, W2, b2)\nprint(f\"Shape of predictions: {predictions.shape}\")\nprint(f\"Shape of y_test: {y_test.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:37:25.058317Z","iopub.execute_input":"2024-06-12T14:37:25.058721Z","iopub.status.idle":"2024-06-12T14:37:25.091930Z","shell.execute_reply.started":"2024-06-12T14:37:25.058691Z","shell.execute_reply":"2024-06-12T14:37:25.090482Z"},"trusted":true},"execution_count":123,"outputs":[{"name":"stdout","text":"Shapes: Z1 -> (4200, 64), A1 -> (4200, 64), Z2 -> (4200, 10), A2 -> (4200, 10)\nShape of predictions: (4200,)\nShape of y_test: (4200,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluate the Model","metadata":{}},{"cell_type":"code","source":"X_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:37:29.107234Z","iopub.execute_input":"2024-06-12T14:37:29.107636Z","iopub.status.idle":"2024-06-12T14:37:29.114793Z","shell.execute_reply.started":"2024-06-12T14:37:29.107608Z","shell.execute_reply":"2024-06-12T14:37:29.113597Z"},"trusted":true},"execution_count":124,"outputs":[{"execution_count":124,"output_type":"execute_result","data":{"text/plain":"(4200, 784)"},"metadata":{}}]},{"cell_type":"code","source":"train_predictions = predict(X_test, W1, b1, W2, b2)\ntrain_accuracy = np.mean(train_predictions == y_test)\nprint(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:37:33.539177Z","iopub.execute_input":"2024-06-12T14:37:33.539595Z","iopub.status.idle":"2024-06-12T14:37:33.572527Z","shell.execute_reply.started":"2024-06-12T14:37:33.539564Z","shell.execute_reply":"2024-06-12T14:37:33.571437Z"},"trusted":true},"execution_count":125,"outputs":[{"name":"stdout","text":"Shapes: Z1 -> (4200, 64), A1 -> (4200, 64), Z2 -> (4200, 10), A2 -> (4200, 10)\nTraining Accuracy: 90.05%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}